{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a196f159-1c6c-4dae-ac45-7fcadc8010e3",
   "metadata": {},
   "source": [
    "# Daily Scheduled Wind DB Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03b642f7-a4e6-43d0-be9e-6b0ca8c819ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FRU] upserted 0 rows (last 48h).\n",
      "[INT] upserted 0 rows (last 48h).\n",
      "[THU] upserted 0 rows (last 48h).\n",
      "Done. Total rows upserted: 0\n"
     ]
    }
   ],
   "source": [
    "# update_wind_pipeline.py\n",
    "\n",
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sqlalchemy import create_engine, text\n",
    "from dateutil import tz\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "def fetch_stations(engine):\n",
    "    q = \"SELECT station_id, COALESCE(local_tz,'Europe/Zurich') AS local_tz FROM stations\"\n",
    "    return pd.read_sql(q, engine)\n",
    "\n",
    "\n",
    "def fetch_recent_csv(station_id: str) -> pd.DataFrame:\n",
    "    st = station_id.lower()\n",
    "    # correct path includes station subdirectory:\n",
    "    url = f\"{BASE}/{st}/ogd-smn_{st}_t_recent.csv\"\n",
    "    r = SESSION.get(url, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return pd.read_csv(io.BytesIO(r.content), sep=\";\", encoding=\"cp1252\")\n",
    "\n",
    "def find_time_col(df):\n",
    "    # Common header names seen in OGD files\n",
    "    candidates = (\"reference_timestamp\")\n",
    "    low = {c.lower(): c for c in df.columns}\n",
    "    for c in candidates:\n",
    "        if c in low:\n",
    "            return low[c]\n",
    "    # fallback to first column\n",
    "    return df.columns[1]\n",
    "\n",
    "def prepare_rows(df_raw, station_tz, local_start=None, local_end=None):\n",
    "    tcol = find_time_col(df_raw)\n",
    "    ts_utc = pd.to_datetime(df_raw[tcol], errors=\"coerce\", dayfirst=True, utc=True)\n",
    "\n",
    "    tz_loc = tz.gettz(station_tz or \"Europe/Zurich\")\n",
    "    ts_local = ts_utc.dt.tz_convert(tz_loc).dt.tz_localize(None)\n",
    "\n",
    "    df = df_raw.assign(tz_utc=ts_utc.dt.tz_convert(\"UTC\").dt.tz_localize(None),\n",
    "                       tz_local=ts_local)\n",
    "\n",
    "    if local_start is not None:\n",
    "        s_local = pd.Timestamp(local_start, tz=tz_loc).tz_convert(tz_loc).tz_localize(None)\n",
    "        df = df[df[\"tz_local\"] >= s_local]\n",
    "    if local_end is not None:\n",
    "        e_local = pd.Timestamp(local_end, tz=tz_loc).tz_convert(tz_loc).tz_localize(None)\n",
    "        df = df[df[\"tz_local\"] <= e_local]\n",
    "\n",
    "    temp_c        = pd.to_numeric(df.get(PAR_TEMP), errors=\"coerce\")\n",
    "    wind_speed_ms = pd.to_numeric(df.get(PAR_WS),   errors=\"coerce\")\n",
    "    wind_dir_deg  = pd.to_numeric(df.get(PAR_WD),   errors=\"coerce\")\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"tz_utc\":   df[\"tz_utc\"].dt.floor(\"10min\"),\n",
    "        \"tz_local\": df[\"tz_local\"].dt.floor(\"10min\"),\n",
    "        \"data_type\": \"observation\",\n",
    "        \"temp_c\":   temp_c.round(2),\n",
    "        \"temp_unit\": \"C\",\n",
    "        \"wind_speed_ms\": wind_speed_ms.round(3),\n",
    "        \"wind_speed_temp_unit\": \"m/s\",\n",
    "        \"wind_dir_deg\":  wind_dir_deg.round(1),\n",
    "        \"source_info\": \"meteoswiss\",\n",
    "    }).dropna(subset=[\"tz_utc\",\"tz_local\",\"wind_speed_ms\",\"wind_dir_deg\"])\n",
    "\n",
    "    return out\n",
    "\n",
    "# compute timestamps from last 48 h back from now\n",
    "def last_48h_window_utc(now_utc=None):\n",
    "    \"\"\"Return (start_utc, end_utc) as naive UTC pandas Timestamps aligned to 10-min grid.\"\"\"\n",
    "    if now_utc is None:\n",
    "        now_utc = pd.Timestamp.utcnow()\n",
    "    end_utc = now_utc.floor(\"10min\")\n",
    "    start_utc = end_utc - pd.Timedelta(hours=48)\n",
    "    # Make them naive (match your tz_utc DATETIME column)\n",
    "    return start_utc.tz_localize(None), end_utc.tz_localize(None)\n",
    "\n",
    "# filter the computed timestamps\n",
    "def filter_last_48h(frame):\n",
    "    if frame.empty:\n",
    "        return frame\n",
    "    start_utc, end_utc = last_48h_window_utc()\n",
    "    # Ensure tz_utc is datetime64[ns] naive\n",
    "    f = frame.copy()\n",
    "    f[\"tz_utc\"] = pd.to_datetime(f[\"tz_utc\"])\n",
    "    return f[(f[\"tz_utc\"] >= start_utc) & (f[\"tz_utc\"] <= end_utc)]\n",
    "\n",
    "def drop_existing_in_window(engine, station_id, frame_48h):\n",
    "    if frame_48h.empty:\n",
    "        return frame_48h\n",
    "    start_utc = frame_48h[\"tz_utc\"].min()\n",
    "    end_utc   = frame_48h[\"tz_utc\"].max()\n",
    "    sql = \"\"\"\n",
    "      SELECT tz_utc\n",
    "      FROM meteo_obs\n",
    "      WHERE station_id = :station_id\n",
    "        AND tz_utc BETWEEN :start_utc AND :end_utc\n",
    "    \"\"\"\n",
    "    existing = pd.read_sql(\n",
    "        text(sql),\n",
    "        engine,  # <- pass engine, not a transactional connection\n",
    "        params={\"station_id\": station_id, \"start_utc\": start_utc, \"end_utc\": end_utc}\n",
    "    )\n",
    "    if existing.empty:\n",
    "        return frame_48h\n",
    "    existing_set = set(pd.to_datetime(existing[\"tz_utc\"]))\n",
    "    return frame_48h[~frame_48h[\"tz_utc\"].isin(existing_set)]\n",
    "\n",
    "# upsert new data in the DB\n",
    "def upsert_obs_last48h(engine, station_id, frame):\n",
    "    f48 = filter_last_48h(frame)\n",
    "    if f48.empty:\n",
    "        return 0\n",
    "    f_new = drop_existing_in_window(engine, station_id, f48)\n",
    "    if f_new.empty:\n",
    "        return 0\n",
    "\n",
    "    f_new = f_new.copy()\n",
    "    f_new[\"station_id\"] = station_id\n",
    "\n",
    "    sql = \"\"\"\n",
    "    INSERT INTO meteo_obs\n",
    "      (station_id, tz_utc, tz_local, data_type,\n",
    "       temp_c, temp_unit, wind_speed_ms, wind_speed_temp_unit, wind_dir_deg, source_info)\n",
    "    VALUES\n",
    "      (:station_id, :tz_utc, :tz_local, :data_type,\n",
    "       :temp_c, :temp_unit, :wind_speed_ms, :wind_speed_temp_unit, :wind_dir_deg, :source_info)\n",
    "    ON DUPLICATE KEY UPDATE\n",
    "       temp_c=VALUES(temp_c),\n",
    "       temp_unit=VALUES(temp_unit),\n",
    "       wind_speed_ms=VALUES(wind_speed_ms),\n",
    "       wind_speed_temp_unit=VALUES(wind_speed_temp_unit),\n",
    "       wind_dir_deg=VALUES(wind_dir_deg),\n",
    "       source_info=VALUES(source_info);\n",
    "    \"\"\"\n",
    "    rows = f_new[[\"station_id\",\"tz_utc\",\"tz_local\",\"data_type\",\n",
    "                  \"temp_c\",\"temp_unit\",\"wind_speed_ms\",\"wind_speed_temp_unit\",\n",
    "                  \"wind_dir_deg\",\"source_info\"]].to_dict(orient=\"records\")\n",
    "\n",
    "    # independent transaction; auto-rollback on exception\n",
    "    from sqlalchemy import text\n",
    "    with engine.begin() as conn:\n",
    "        for i in range(0, len(rows), 800):\n",
    "            conn.execute(text(sql), rows[i:i+800])\n",
    "    return len(f_new)\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "engine = create_engine(\n",
    "    f\"mysql+pymysql://{os.getenv('DB_USER')}:{os.getenv('DB_PASS')}@\"\n",
    "    f\"{os.getenv('DB_HOST')}:{int(os.getenv('DB_PORT', 3306))}/{os.getenv('DB_NAME')}\")\n",
    "\n",
    "# Optional: limit by local time window (e.g., this year to today); leave as None to load all \"recent\"\n",
    "LOCAL_START = \"2025-02-01 00:00\"   # Europe/Zurich (or None)\n",
    "LOCAL_END   = None\n",
    "\n",
    "# Meteoswiss CONSTANTS \n",
    "BASE = \"https://data.geo.admin.ch/ch.meteoschweiz.ogd-smn\"\n",
    "PAR_TEMP = \"tre200s0\"   # Â°C\n",
    "PAR_WS   = \"fve010z0\"   # m/s (requested)\n",
    "PAR_WD   = \"dkl010z0\"   # degrees (from-direction)\n",
    "\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\n",
    "    \"User-Agent\": \"wind-pipeline/1.0 (+your.email@example.com)\",\n",
    "    \"Accept\": \"text/csv,*/*;q=0.1\"})\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "# ==== RUN DAILYUPDATE (48h only) ====\n",
    "stations = fetch_stations(engine)\n",
    "total = 0\n",
    "for _, row in stations.iterrows():\n",
    "    st = row[\"station_id\"]\n",
    "    tz_name = row.get(\"local_tz\") or \"Europe/Zurich\"\n",
    "    try:\n",
    "        raw = fetch_recent_csv(st)\n",
    "        prepared = prepare_rows(raw, tz_name, local_start=None, local_end=None)\n",
    "        n = upsert_obs_last48h(engine, st, prepared)\n",
    "        print(f\"[{st}] upserted {n} rows (last 48h).\")\n",
    "        total += n\n",
    "    except Exception as e:\n",
    "        print(f\"[{st}] ERROR: {e}\")\n",
    "        # Optional: drop tainted connections from pool after a DB error\n",
    "        engine.dispose()\n",
    "print(f\"Done. Total rows upserted: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f492391-8e93-4903-98b9-d3ac64c265bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Webcamscraper)",
   "language": "python",
   "name": "webcamscraper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
